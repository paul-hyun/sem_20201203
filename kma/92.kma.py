# -*- coding: utf-8 -*-
"""kma.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CKJ0TVeE8eDGrNhy_rwrt58kc_9pEGij
"""

import os

import pandas as pd
import numpy as np
import tensorflow as tf
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# google drive mount
from google.colab import drive
drive.mount('/content/drive')

data_dir = '/content/drive/MyDrive/Colab Notebooks/TimeSeriese/kma'
os.listdir(data_dir)

train_start = pd.to_datetime('1904-04-01')
train_end = pd.to_datetime('2001-12-31 ')
test_start = pd.to_datetime('2010-01-01 ')
test_end = pd.to_datetime('2020-10-31 ')
train_start, test_end

df = pd.read_csv(os.path.join(data_dir, 'OBS_ASOS_MNH_20201123233109.csv'), encoding='CP949')
df

df['일시'] = pd.to_datetime(df['일시'], format='%Y-%m')
df = df[(train_start <= df['일시']) & (df['일시'] <= test_end)]
df

df['월'] = df["일시"].dt.month

df_test = df[(test_start <= df["일시"]) & (df["일시"] <= test_end)]
test_len = len(df_test['일시'].unique())
test_len

df.info()

# x_cols = ['평균기온(°C)']
x_cols = ['평균기온(°C)', '최고기온(°C)', '최저기온(°C)', '평균현지기압(hPa)', '평균해면기압(hPa)', '평균수증기압(hPa)', '평균이슬점온도(°C)', '평균상대습도(%)', '월합강수량(00~24h만)(mm)', '평균풍속(m/s)', '평균운량(1/10)', '일조율(%)', '월']
y_col = '평균기온(°C)'

df_value = df[['지점', '지점명', '일시'] + x_cols]
df_value

df_value.isnull().sum()

df_value.info()

df_value.describe()

location_keys = df_value[['지점', '지점명']].values
location_keys = [tuple(x) for x in location_keys]
location_keys = list(dict.fromkeys(location_keys))
len(location_keys), location_keys

y_index = 0
for i, (code, code_name) in enumerate(location_keys):
    if code_name == '수원':
        y_index = i
        break
y_index, location_keys[y_index]

pivot_dic = {}
for col in x_cols:
    df_pivot = df_value.pivot_table(index = ['지점'], values = col, columns = ['일시'], aggfunc='sum')
    df_pivot = df_pivot.sort_index()
    df_pivot = df_pivot.reset_index()
    pivot_dic[col] = df_pivot
len(pivot_dic)

pivot_dic[x_cols[0]]

pivot_dic[x_cols[-1]]

locations = pivot_dic[x_cols[0]]['지점'].unique()
locations

for col, df_pivot in pivot_dic.items():
    for location in tqdm(locations):
        mean = df_pivot.loc[df_pivot['지점'] == location].mean(axis=1).to_numpy()[0]
        df_pivot.loc[df_pivot['지점'] == location] = df_pivot.loc[df_pivot['지점'] == location].fillna(mean)

pivot_dic[x_cols[0]]

pivot_dic[x_cols[0]].isnull().sum(axis=1)

pivot_dic[x_cols[-1]]

pivot_dic[x_cols[-1]].isnull().sum(axis=1)

class MinMaxScaler():
    def __init__(self, min_val, max_val):
        assert (max_val > min_val)
        self.min_val = min_val
        self.max_val = max_val

    def scale_value(self, val):
        return (val - self.min_val) / (self.max_val - self.min_val)

    def inv_scale_value(self, scaled_val):
        return self.min_val + scaled_val * (self.max_val - self.min_val)

scaler_list = []
matrix_list = []
for col, df_pivot in pivot_dic.items():
    print(f'{col}: {df_pivot.values.shape}')
    matrix = df_pivot.values
    scaler_list.append(MinMaxScaler(np.min(matrix), np.max(matrix)))
    matrix_list.append(matrix)

matrix_scaled_list = []
for scaler, matrix in zip(scaler_list, matrix_list):
    matrix_scaled = np.zeros_like(matrix)
    bs, n_total = matrix_scaled.shape
    for i in range(bs):
        for j in range(n_total):
            matrix_scaled[i][j] = scaler.scale_value(matrix[i][j])
    matrix_scaled_list.append(matrix_scaled)

data_matrix = np.stack(matrix_scaled_list, axis=-1)
bs, n_total, d_model = data_matrix.shape
bs, n_total, d_model

n_seq = 24
total_inputs, total_labels = [], []
for i in range(n_total - n_seq):
    x = data_matrix[:, i:i + n_seq]
    y = data_matrix[:, i + n_seq, :1]
    total_inputs.append(x)
    total_labels.append(y)

len(total_inputs), len(total_labels)

train_inputs = total_inputs[:-test_len + n_seq]
train_labels = total_labels[:-test_len + n_seq]
len(train_inputs), len(train_labels)

test_inputs = total_inputs[-test_len + n_seq:]
test_labels = total_labels[-test_len + n_seq:]
len(test_inputs), len(test_labels)

train_inputs = np.concatenate(train_inputs, axis=0)
train_labels = np.concatenate(train_labels, axis=0)
train_inputs.shape, train_labels.shape

test_inputs = [v[y_index:y_index+1] for v in test_inputs]
test_labels = [v[y_index:y_index+1] for v in test_labels]

test_inputs = np.concatenate(test_inputs, axis=0)
test_labels = np.concatenate(test_labels, axis=0)
test_inputs.shape, test_labels.shape

def build_model_rnn(n_seq, d_model):
    inputs = tf.keras.layers.Input((n_seq, d_model))  # bs, n_seq, d_model

    hidden = tf.keras.layers.Conv1D(filters=64, kernel_size=6, padding='causal')(inputs)
    hidden = tf.keras.layers.LSTM(units=256, activation=tf.nn.relu)(hidden)  # (bs, units)
    hidden = tf.keras.layers.Dropout(0.4)(hidden)

    output_dense = tf.keras.layers.Dense(1, activation=tf.nn.tanh)
    outputs = output_dense(hidden)

    model = tf.keras.Model(inputs=(inputs), outputs=outputs)
    return model

model = build_model_rnn(n_seq, d_model)

model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer =tf.keras.optimizers.Adam())
model.summary()

# early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)
# save weights
save_weights = tf.keras.callbacks.ModelCheckpoint('weights.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min', save_freq='epoch', save_weights_only=True)

model.fit(train_inputs, train_labels, epochs = 500, batch_size = 4096, validation_data=(test_inputs, test_labels), callbacks=[early_stopping, save_weights])

model = build_model_rnn(n_seq, d_model)
model.load_weights('weights.hdf5')

scaler = scaler_list[0]

y_pred = model.predict(test_inputs)
y_pred = np.squeeze(y_pred, axis=-1)
y_pred = np.array([scaler.inv_scale_value(v) for v in y_pred])
y_pred

y_true = np.squeeze(test_labels, axis=-1)
y_true = np.array([scaler.inv_scale_value(v) for v in y_true])
y_true

plt.figure(figsize=(16, 4))
plt.plot(y_true, 'b-', label='y_true')
plt.plot(y_pred, 'r--', label='y_pred')
plt.legend()
plt.show()

plt.figure(figsize=(16, 4))
plt.plot(y_true - y_pred, 'g-', label='y_diff')
plt.legend()
plt.show()

rmse = tf.sqrt(tf.keras.losses.MSE(y_true, y_pred))
mae = tf.keras.losses.MAE(y_true, y_pred)
mape = tf.keras.losses.MAPE(y_true, y_pred)
pd.DataFrame([rmse, mae, mape], index=['RMSE', 'MAE', 'MAPE'])

